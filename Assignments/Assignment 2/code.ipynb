{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Device: cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Device: cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple, Union, Any\n",
    "import sys\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import (\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    ModelSummary,\n",
    ")\n",
    "from rich import print\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rich\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "L.seed_everything(seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Set data directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset class for Shakespeare text data.\n",
    "\n",
    "    Attributes:\n",
    "        text (str): The entire text from the file.\n",
    "        chars (List[str]): A sorted list of unique characters in the text.\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        s_to_i (Dict[str, int]): A dictionary mapping characters to indices.\n",
    "        i_to_s (Dict[int, str]): A dictionary mapping indices to characters.\n",
    "        seq_len (int): The length of the sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, seq_len: int):\n",
    "        \"\"\"\n",
    "        Initializes the ShakespeareDataset with the given file path and sequence length.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the text file.\n",
    "            seq_len (int): Length of the sequences.\n",
    "        \"\"\"\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        self.text = text\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.s_to_i = {s: i for i, s in enumerate(self.chars)}\n",
    "        self.i_to_s = {i: s for i, s in enumerate(self.chars)}\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.text) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a tuple of input and target sequences for the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sequence.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Input and target sequences as tensors.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            torch.tensor(\n",
    "                [self.s_to_i[c] for c in self.text[idx : idx + self.seq_len]],\n",
    "                dtype=torch.long,\n",
    "            ),\n",
    "            torch.tensor(\n",
    "                [self.s_to_i[c] for c in self.text[idx + 1 : idx + self.seq_len + 1]],\n",
    "                dtype=torch.long,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def decode(self, x: Union[torch.Tensor, List[int]]) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a tensor or list of indices back to a string.\n",
    "\n",
    "        Args:\n",
    "            x (Union[torch.Tensor, List[int]]): Tensor or list of indices.\n",
    "\n",
    "        Returns:\n",
    "            str: Decoded string.\n",
    "        \"\"\"\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.tolist()\n",
    "\n",
    "        return \"\".join([self.i_to_s[i] for i in x])\n",
    "\n",
    "    def encode(self, s: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a string into a tensor of indices.\n",
    "\n",
    "        Args:\n",
    "            s (str): Input string.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded tensor of indices.\n",
    "        \"\"\"\n",
    "        encoded = [self.s_to_i[c] for c in s]\n",
    "        return torch.tensor(encoded, dtype=torch.long).to(device)\n",
    "\n",
    "    def collate_fn(\n",
    "        self, batch: List[Tuple[torch.Tensor, torch.Tensor]]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Collates a batch of sequences into padded tensors.\n",
    "\n",
    "        Args:\n",
    "            batch (List[Tuple[torch.Tensor, torch.Tensor]]): Batch of input and target sequences.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Padded input and target sequences.\n",
    "        \"\"\"\n",
    "        x, y = zip(*batch)\n",
    "        x = torch.nn.utils.rnn.pad_sequence(\n",
    "            x, batch_first=True, padding_value=self.s_to_i[\" \"]\n",
    "        )\n",
    "        y = torch.nn.utils.rnn.pad_sequence(\n",
    "            y, batch_first=True, padding_value=self.s_to_i[\" \"]\n",
    "        )\n",
    "        return x, y\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the size of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: Vocabulary size.\n",
    "        \"\"\"\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RNN/LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_layers: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ShakespeareModel.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of the embedding layer.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            num_layers (int): Number of LSTM layers.\n",
    "        \"\"\"\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model.\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            else:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x, hc = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def generate(self, x: torch.Tensor, n: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates a sequence of tokens.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            n (int): Number of tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Generated sequence.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        for _ in range(n):\n",
    "            logits = self(x)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x, idx_next], dim=1)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: ShakespeareDataset,\n",
    "        model_hparams: Dict[str, Any],\n",
    "        other_hparams: Dict[str, Any],\n",
    "        optimizer_name: str,\n",
    "        optimizer_hparams: Dict[str, Any],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ShakespeareModule.\n",
    "\n",
    "        Args:\n",
    "            dataset (ShakespeareDataset): The dataset to be used.\n",
    "            model_hparams (Dict[str, Any]): Hyperparameters for the model.\n",
    "            other_hparams (Dict[str, Any]): Other hyperparameters.\n",
    "            optimizer_name (str): Name of the optimizer to be used.\n",
    "            optimizer_hparams (Dict[str, Any]): Hyperparameters for the optimizer.\n",
    "        \"\"\"\n",
    "        super(ShakespeareModule, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.model = ShakespeareModel(**model_hparams)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.example_input_array = torch.randint(0, 10, (1, 32))  # dummy input\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Configures the optimizers and learning rate scheduler.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Dictionary containing the optimizer and scheduler.\n",
    "        \"\"\"\n",
    "        if self.hparams.optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(), **self.hparams.optimizer_hparams\n",
    "            )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=10, verbose=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (Tuple[torch.Tensor, torch.Tensor]): Batch of data.\n",
    "            batch_idx (int): Index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Loss value.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self(x)\n",
    "\n",
    "        # Reshape logits and labels to (N, C) shape for loss function\n",
    "        loss = self.loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (Tuple[torch.Tensor, torch.Tensor]): Batch of data.\n",
    "            batch_idx (int): Index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Loss value.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self(x)\n",
    "\n",
    "        # Reshape logits and labels to (N, C) shape for loss function\n",
    "        loss = self.loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            predicted = self.generate_text(x, n=100)\n",
    "\n",
    "            predicted_text = self.dataset.decode(predicted[0])\n",
    "\n",
    "            self.logger.experiment.add_text(\n",
    "                \"input_text\", self.dataset.decode(x[0]), self.current_epoch\n",
    "            )\n",
    "            self.logger.experiment.add_text(\n",
    "                \"val_text\", predicted_text, self.current_epoch\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def generate_text(self, x: torch.Tensor, n: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates text using the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            n (int): Number of tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Generated text tensor.\n",
    "        \"\"\"\n",
    "        return self.model.generate(x, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 16\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "s = ShakespeareDataset(\"./input.txt\", seq_len=SEQ_LEN)\n",
    "\n",
    "L.seed_everything(seed)\n",
    "\n",
    "# Train-val split\n",
    "train_data, val_data = torch.utils.data.random_split(\n",
    "    s, [int(0.9 * len(s)), len(s) - int(0.9 * len(s))]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input: :\n",
       "Marry, sir, 't\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Input: :\n",
       "Marry, sir, 't\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Target: \n",
       "Marry, sir, 'ti\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Target: \n",
       "Marry, sir, 'ti\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input:  stand and look \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Input:  stand and look \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Target: stand and look u\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Target: stand and look u\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input: tagenet, root hi\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Input: tagenet, root hi\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Target: agenet, root him\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Target: agenet, root him\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input: ding now from hi\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Input: ding now from hi\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Target: ing now from him\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Target: ing now from him\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"Input: {s.decode(x[i])}\")\n",
    "    print(f\"Target: {s.decode(y[i])}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Number of parameters: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1472961</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Number of parameters: \u001b[1;36m1472961\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ShakespeareModule(\n",
    "    dataset=s,\n",
    "    model_hparams={\n",
    "        \"vocab_size\": s.get_vocab_size(),\n",
    "        \"embedding_dim\": 128,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"num_layers\": 3,\n",
    "    },\n",
    "    other_hparams={\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "    },\n",
    "    optimizer_name=\"adam\",\n",
    "    optimizer_hparams={\"lr\": 1e-3, \"weight_decay\": 1e-6},\n",
    ")\n",
    "\n",
    "# Parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=50,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\"),\n",
    "        LearningRateMonitor(\n",
    "            logging_interval=\"epoch\", log_momentum=True, log_weight_decay=True\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer.logger._log_graph = True\n",
    "trainer.logger._default_hp_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:376: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "\n",
      "  | Name    | Type             | Params | In sizes | Out sizes  \n",
      "----------------------------------------------------------------------\n",
      "0 | model   | ShakespeareModel | 1.5 M  | [1, 32]  | [1, 32, 65]\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | ?        | ?          \n",
      "----------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.892     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac33b5014fc042378b9e79aefe56875f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eec37162bf943eb9b76a4ac69b11d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccb7bb2fea649d19c3b5b7424f8cdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4650af902d041a080ad9f246c23e284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6fec24e22644389ed443218b024f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56715d164a241b1b5cf507e424e76ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683d1171380f4a3fb92b7999689867b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef9b645a5d7447d9884a21a50289b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20f6b2df9b94044b866ad305018d7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f242f76ad8364307a489e947d37080ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb554e81b794f93afcd83d90d27a683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ff24854aae46afb4601349c71bec1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa291df871104b65a921899c5b0954bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca61295e956145c7b86924238448266b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6d258e5f3b4aec8a3a9b19cbb73692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7878f876adc44b29a38021250528f674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506bd55492904107ba12c91a63e764e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fff62e88d348e281e0ad849bab194f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b72929cd234c6faf80b1cbc612c3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3a85d10a7e4742985822b0c253c073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90339f233a74d0194aea07a1d47ee4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa912ff68ecd4b97806332da851dd48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2667b9b27ecc42cba63f926125198ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0410356468a489b88f66c1093a103b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5c28b521a1445ca7ef172f00d6b789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "L.seed_everything(42)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = ShakespeareModule.load_from_checkpoint(\n",
    "    \"./lightning_logs/version_1/checkpoints/epoch=21-step=172546.ckpt\",\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    logger=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044a64415a6d44689ba8553fe1af5425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2201244831085205     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2201244831085205    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'val_loss_epoch'\u001b[0m: \u001b[1;36m1.2201244831085205\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "will affliction will draw them and he, precious!\n",
      "\n",
      "CORIOLANUS:\n",
      "Fellow, she is not one.\n",
      "\n",
      "POLIXENES:\n",
      "To you, mispredition man, more than this again; come, sit down;\n",
      "For having hours discreeth,\n",
      "Whose case is marvellous chance to make the cold; by the said was Bohemia; thou hast masquing blush and presence at the matter.\n",
      "\n",
      "MENENIUS:\n",
      "A hundred thousand thoughts of yonder: if things expect'd?\n",
      "\n",
      "DUKE OF YORK:\n",
      "Lay her volubtle, sir, and you hope I see the assembling slave:\n",
      "But that I want you do protest.\n",
      "\n",
      "Second Watchman:\n",
      "Sovery one shoeld\n",
      "In thy happy buried, this strokes.\n",
      "\n",
      "TYBALT:\n",
      "An I do find break out forth the war; but then I'ld be writ,\n",
      "Lest base to complain accused leisure but underneath the king\n",
      "As myself in trouble him\n",
      "To look on me! then Clarence issued and end\n",
      "With thission of the people.\n",
      "\n",
      "CORIOLANUS:\n",
      "Indeaver, upon your voices by guilty, counsel and a fool\n",
      "Is bore this design. Let's clove.\n",
      "\n",
      "FRIAR POLIXENES:\n",
      "O,\n",
      "that your hands,\n",
      "What if it be so straight\n",
      "A horses' eyes ot blow: your f"
     ]
    }
   ],
   "source": [
    "# Sample text generation from model\n",
    "model.eval()\n",
    "x = s.encode(\"ANTONIO: \").unsqueeze(0)\n",
    "\n",
    "MAX_N = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(MAX_N):\n",
    "        logits = model(x)[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        sys.stdout.write(s.decode(idx_next[0]))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        x = torch.cat([x, idx_next], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
