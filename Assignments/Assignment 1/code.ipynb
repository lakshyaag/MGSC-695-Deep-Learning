{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    ModelSummary,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from rich import print\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict\n",
    "import tensorboard\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rich\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "L.seed_everything(seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Set data directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(DATA_DIR, train=True, download=True)\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some images randomly\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, idx in enumerate(np.random.choice(len(train_dataset), 5, replace=False)):\n",
    "    ax[i].imshow(train_dataset[idx][0])\n",
    "    ax[i].set_title(f\"Label: {train_dataset[idx][1]}\")\n",
    "    ax[i].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0, 1, 2))\n",
    "DATA_STD = (train_dataset.data / 255.0).std(axis=(0, 1, 2))\n",
    "print(f\"Data mean: {DATA_MEANS}, Data std: {DATA_STD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(DATA_MEANS, DATA_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(DATA_MEANS, DATA_STD),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CIFAR-10 dataset and split it into train and validation sets\n",
    "\n",
    "train_dataset = CIFAR10(DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "val_dataset = CIFAR10(DATA_DIR, train=True, download=True, transform=test_transform)\n",
    "test_dataset = CIFAR10(DATA_DIR, train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Generate validation set without any data leakage while applying different transformations\n",
    "L.seed_everything(seed)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "L.seed_everything(seed)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "# Generate data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 4\n",
    "RNG_IDX = np.random.choice(len(train_dataset), NUM_IMAGES)\n",
    "images = [train_dataset[idx][0] for idx in RNG_IDX]\n",
    "orig_images = [Image.fromarray(train_dataset.data[idx]) for idx in RNG_IDX]\n",
    "orig_images = [test_transform(img) for img in orig_images]\n",
    "\n",
    "img_grid = make_grid(\n",
    "    torch.stack(images + orig_images, dim=0), nrow=4, normalize=True, pad_value=0.5\n",
    ")\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Augmentation examples on CIFAR10\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, first_stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=first_stride,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=out_channels),\n",
    "        )\n",
    "\n",
    "        if first_stride > 1:\n",
    "            self.right = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=first_stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(num_features=out_channels),\n",
    "            )\n",
    "        else:\n",
    "            assert (\n",
    "                in_channels == out_channels\n",
    "            ), \"in_channels must be equal to out_channels\"\n",
    "            self.right = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        left = self.left(x)\n",
    "        right = self.right(x)\n",
    "        return self.relu(left + right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGroup(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_blocks: int, in_channels: int, out_channels: int, first_stride: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels, first_stride=first_stride),\n",
    "            *[ResidualBlock(out_channels, out_channels) for _ in range(n_blocks - 1)],\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        \"\"\"\n",
    "        return self.blocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks_per_group: List[int],\n",
    "        out_features_per_group: List[int],\n",
    "        first_strides_per_group: List[int],\n",
    "        n_classes: int = 10,\n",
    "        in_feats0: int = 16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_feats0 = in_feats0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_blocks_per_group = n_blocks_per_group\n",
    "        self.out_features_per_group = out_features_per_group\n",
    "        self.first_strides_per_group = first_strides_per_group\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, in_feats0, kernel_size=1, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=in_feats0),\n",
    "        )\n",
    "\n",
    "        all_in_feats = [in_feats0] + out_features_per_group[:-1]\n",
    "\n",
    "        self.residual_layers = nn.Sequential(\n",
    "            *(\n",
    "                BlockGroup(*args)\n",
    "                for args in zip(\n",
    "                    n_blocks_per_group,\n",
    "                    all_in_feats,\n",
    "                    out_features_per_group,\n",
    "                    first_strides_per_group,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.out_layers = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(out_features_per_group[-1], n_classes),\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.out_layers(self.residual_layers(self.in_layers(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_hparams: Dict,\n",
    "        optimizer_name: str,\n",
    "        optimizer_hparams: Dict,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = ResNet(**model_hparams)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer_name == \"Adam\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(), **self.hparams.optimizer_hparams\n",
    "            )\n",
    "\n",
    "        elif self.hparams.optimizer_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(\n",
    "                self.parameters(), **self.hparams.optimizer_hparams\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            assert False, f\"Unknown optimizer: {self.hparams.optimizer_name}\"\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[100, 150], gamma=0.1\n",
    "        )\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(\n",
    "            \"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CIFARModule(\n",
    "    model_hparams={\n",
    "        \"n_blocks_per_group\": [3, 3, 3],\n",
    "        \"out_features_per_group\": [16, 32, 64],\n",
    "        \"first_strides_per_group\": [1, 2, 2],\n",
    "        \"n_classes\": 10,\n",
    "    },\n",
    "    optimizer_name=\"SGD\",\n",
    "    optimizer_hparams={\"lr\": 0.1, \"weight_decay\": 1e-4, \"momentum\": 0.9},\n",
    ")\n",
    "\n",
    "# 6n + 2 layers\n",
    "print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=10,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "        LearningRateMonitor(\n",
    "            logging_interval=\"epoch\", log_momentum=True, log_weight_decay=True\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer.logger._log_graph = True\n",
    "trainer.logger._default_hp_metric = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(42)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result = trainer.test(model, dataloaders=val_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
