# Assignment 3 - Project: Multi-class Text Classification using Transformers (Duration: 2 weeks)

## Objective

The aim of this project is to provide a comprehensive experience of working with Transformers, from understanding the theory to implementing them in practice. You'll work on a multi-class text classification task, a common and highly useful task in Natural Language Processing (NLP).

## Dataset

20 Newsgroups dataset. This dataset contains approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups.

## Tasks

- Preprocess the data: The text data needs to be tokenized, and possibly further preprocessed by lowercasing and removing punctuation. Convert your labels into a format suitable for multi- class classification.
- Implement a Transformer model: You can use a pre-trained transformer model like BERT or GPT, and fine-tune it for your specific task. Make sure to correctly implement the self- attention mechanism.
- Train your model: Train the model using your processed data. Implement a mechanism to save the weights of the model periodically or when it achieves the best performance on a validation set.
- Evaluate your model: Evaluate the performance of your model on a held-out test set. Provide a report on your model's performance using metrics such as accuracy, precision, recall, and F1 score for each class, as well as averaged over all classes.

## Success Criteria

- Successful implementation and fine-tuning of a transformer model for multi-class text classification.
- Achieving reasonable performance on the test set. Compare your results with reported benchmarks using the same dataset.
- Clear documentation of your data preprocessing steps, model architecture, training process, and evaluation results. Highlight any difficulties encountered and how you addressed them.
-Successful implementation of the self-attention mechanism, and a clear explanation of how it works in your model and why it's beneficial for the task.
