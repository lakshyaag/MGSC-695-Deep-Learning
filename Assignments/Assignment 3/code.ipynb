{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Device: cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Device: cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple, Union, Any\n",
    "import sys\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import (\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    ModelSummary,\n",
    ")\n",
    "import copy\n",
    "from rich import print\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, AutoModelForSequenceClassification, GPT2Config\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rich\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "L.seed_everything(seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Set data directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(data_home=DATA_DIR, subset=\"train\")\n",
    "test = fetch_20newsgroups(data_home=DATA_DIR, subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m11314\u001b[0m, \u001b[1;36m7532\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.data), len(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, target, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.target[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long).to(device),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mGPT2Tokenizer\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'gpt2'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50257\u001b[0m, \u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'left'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'eos_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'unk_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'pad_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|>'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m, \u001b[0m\u001b[33mclean_up_tokenization_spaces\u001b[0m\u001b[39m=\u001b[0m\u001b[3;92mTrue\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,  \u001b[0m\u001b[33madded_tokens_decoder\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;36m50256\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mAddedToken\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"<|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\"\u001b[0m, \u001b[33mrstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33msingle_word\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mnormalized\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mspecial\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train.data, train.target, tokenizer, max_length=1024)\n",
    "test_dataset = TextDataset(test.data, test.target, tokenizer, max_length=1024)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24517b24b6a44c7bb8dfe3f152ee90df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "cfg = GPT2Config.from_pretrained(\"gpt2\", num_labels=20)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", config=cfg)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mGPT2ForSequenceClassification\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mtransformer\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Model\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mwte\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mwpe\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdrop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mh\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m11\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m12\u001b[0m x \u001b[1;35mGPT2Block\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mln_1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Attention\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mattn_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mresid_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mln_2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mGPT2MLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_fc\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mNewGELUActivation\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln_f\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mscore\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Classifier(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super(GPT2Classifier, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = self(input_ids, attention_mask, labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = self(input_ids, attention_mask, labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=2e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GPT2Classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    logger=False,\n",
    "    max_epochs=2,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"val_loss\"),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building GPT-2 from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1D\n",
    "\n",
    "This is essentially a linear transformation applied using a 1D convolutional layer to provide greater flexibility in the model as the flattening happens automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        w = torch.empty(input_dim, output_dim)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        x: (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        size_out = x.size()[:-1] + (self.weight.size(1),)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "\n",
    "        x = x.view(*size_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv1D(768, 20)(torch.randn(8, 1024, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward network\n",
    "\n",
    "This feed-forward network takes in the pre-activation values from the attention block, projects them to a higher-dimensional space, applies a non-linearity, and then projects them back down to the original dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = Conv1D(input_dim, output_dim)\n",
    "        self.c_proj = Conv1D(output_dim, input_dim)\n",
    "\n",
    "        self.act = F.gelu\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.act(self.c_fc(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFN(768, 768 * 4)(torch.randn(8, 1024, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, n_ctx, d_head, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "\n",
    "        self.c_attn = Conv1D(d_model, d_model * 3)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"tril\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.c_proj = Conv1D(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        x: (batch_size, n_head, seq_len, d_head)\n",
    "        \"\"\"\n",
    "\n",
    "        new_shape = x.size()[:-1] + (\n",
    "            self.n_head,\n",
    "            self.d_head,\n",
    "        )  # (batch_size, seq_len, n_head, d_head)\n",
    "        x = x.view(*new_shape)\n",
    "\n",
    "        return x.permute(0, 2, 1, 3)  # (batch_size, n_head, seq_len, d_head)\n",
    "\n",
    "    def _attn(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q: (batch_size, n_head, seq_len, d_head)\n",
    "        k: (batch_size, n_head, seq_len, d_head)\n",
    "        v: (batch_size, n_head, seq_len, d_head)\n",
    "\n",
    "        Returns:\n",
    "        out: (batch_size, n_head, seq_len, d_head)\n",
    "        \"\"\"\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "        scores = scores / np.sqrt(v.size(-1))  # (batch_size, n_head, seq_len, seq_len)\n",
    "\n",
    "        if mask is None:\n",
    "            mask = (\n",
    "                torch.tril(torch.ones(scores.size(-2), scores.size(-1)))\n",
    "                .view(1, 1, *scores.size()[-2:])\n",
    "                .to(scores.device)\n",
    "            )\n",
    "\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = self.softmax(scores)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, n_head, seq_len, d_head)\n",
    "\n",
    "        Returns:\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
    "        x = x.view(*new_shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.c_attn(x)\n",
    "\n",
    "        q, k, v = x.split(self.d_model, dim=-1)\n",
    "        q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "\n",
    "        out = self._attn(q, k, v, mask)\n",
    "        out = self.merge_heads(out)\n",
    "\n",
    "        out = self.c_proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention(768, 8, 1024, 768 // 8)(torch.randn(8, 1024, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = Attention(\n",
    "            d_model, n_head, n_ctx=1024, d_head=d_model // n_head, bias=True\n",
    "        )\n",
    "        self.mlp = FFN(d_model, d_model * 4, dropout=dropout)\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(d_model)\n",
    "        self.ln_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.attn(self.ln_1(x), mask=mask)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformerBlock(768, 12)(torch.randn(8, 1024, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_layers=12, n_head=12, n_ctx=1024, d_model=768, n_labels=20, vcb_sz=50257\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wte = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe = nn.Embedding(n_ctx, d_model)\n",
    "\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "        block = TransformerBlock(d_model=d_model, n_head=n_head, dropout=0.1)\n",
    "        self.h = self._get_clones(block, n_layers)\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.out = nn.Linear(d_model, n_labels, bias=False)\n",
    "\n",
    "        self._init_weights(self)\n",
    "\n",
    "    def _get_clones(self, module, n):\n",
    "        return nn.ModuleList([copy.deepcopy(module) for i in range(n)])\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, src, labels=None, mask=None, pos_ids=None):\n",
    "        \"\"\"\n",
    "        src: (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        logits: (batch_size, n_labels)\n",
    "        \"\"\"\n",
    "        if pos_ids is None:\n",
    "            pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0).to(src.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inp = self.drop((self.wte(src) + self.wpe(pos_ids)))\n",
    "\n",
    "            for i in range(self.n_layers):\n",
    "                inp = self.h[i](inp, mask=mask)\n",
    "\n",
    "            inp = self.ln_f(inp)\n",
    "\n",
    "        logits = self.out(inp[:, -1])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2(2, 12, 1024, 768, 20, 1000)(torch.randint(0, 1000, (8, 1024))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mGPT2\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mwte\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mwpe\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdrop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mh\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m x \u001b[1;35mTransformerBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0msoftmax\u001b[1m)\u001b[0m: \u001b[1;35mSoftmax\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdim\u001b[0m=\u001b[1;36m-1\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mFFN\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mc_fc\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdrop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mln_1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mln_2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mln_f\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mout\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_gpt2 = GPT2(3, 12, 1024, 768, 20, 50257).to(device)\n",
    "\n",
    "custom_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mGPT2ForSequenceClassification\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mtransformer\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Model\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mwte\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mwpe\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdrop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mh\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m x \u001b[1;35mGPT2Block\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mln_1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Attention\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mattn_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mresid_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mln_2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mGPT2MLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_fc\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mNewGELUActivation\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln_f\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mscore\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = GPT2Config.from_pretrained(\"gpt2\", num_labels=20, n_layer=3)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", config=cfg)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "custom_gpt2_state_dict = custom_gpt2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mGPT2\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mwte\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mwpe\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdrop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mh\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m x \u001b[1;35mTransformerBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0msoftmax\u001b[1m)\u001b[0m: \u001b[1;35mSoftmax\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdim\u001b[0m=\u001b[1;36m-1\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mFFN\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mc_fc\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdrop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mln_1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mln_2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mln_f\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mout\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_keys = []\n",
    "new_keys = []\n",
    "\n",
    "for key in state_dict.keys():\n",
    "    if \"transformer\" in key:\n",
    "        old_keys.append(key)\n",
    "        new_keys.append(key.replace(\"transformer.\", \"\"))\n",
    "\n",
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "pretrained_dict = {k: v for k, v in state_dict.items() if k in custom_gpt2_state_dict}\n",
    "\n",
    "custom_gpt2_state_dict.update(pretrained_dict)\n",
    "\n",
    "custom_gpt2.load_state_dict(custom_gpt2_state_dict)\n",
    "custom_gpt2.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m6930\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m628\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m,  \u001b[33m...\u001b[0m,    \u001b[1;36m13\u001b[0m, \u001b[1;36m15532\u001b[0m,   \u001b[1;36m198\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m,  \u001b[33m...\u001b[0m,   \u001b[1;36m628\u001b[0m,   \u001b[1;36m628\u001b[0m,   \u001b[1;36m198\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33m...\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m19618\u001b[0m,    \u001b[1;36m29\u001b[0m,   \u001b[1;36m198\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m,  \u001b[33m...\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m14679\u001b[0m,   \u001b[1;36m198\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m, \u001b[1;36m50256\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m4992\u001b[0m, \u001b[1;36m23500\u001b[0m,   \u001b[1;36m198\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_gpt2.train()\n",
    "logits = custom_gpt2(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-2.0713e+00\u001b[0m,  \u001b[1;36m1.2661e+00\u001b[0m,  \u001b[1;36m8.9001e-01\u001b[0m, \u001b[1;36m-1.8227e-01\u001b[0m, \u001b[1;36m-3.4259e-01\u001b[0m,\n",
       "          \u001b[1;36m1.6165e+00\u001b[0m, \u001b[1;36m-1.3114e-01\u001b[0m,  \u001b[1;36m1.6069e-01\u001b[0m,  \u001b[1;36m1.1070e+00\u001b[0m,  \u001b[1;36m2.8233e-01\u001b[0m,\n",
       "          \u001b[1;36m5.8022e-02\u001b[0m, \u001b[1;36m-9.4528e-01\u001b[0m,  \u001b[1;36m4.9517e-01\u001b[0m, \u001b[1;36m-8.8699e-01\u001b[0m, \u001b[1;36m-9.2799e-01\u001b[0m,\n",
       "          \u001b[1;36m8.8554e-01\u001b[0m, \u001b[1;36m-5.4683e-01\u001b[0m, \u001b[1;36m-9.4678e-01\u001b[0m, \u001b[1;36m-5.8822e-02\u001b[0m, \u001b[1;36m-7.6902e-02\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-2.2478e+00\u001b[0m,  \u001b[1;36m1.6846e+00\u001b[0m,  \u001b[1;36m1.6972e+00\u001b[0m,  \u001b[1;36m5.4963e-01\u001b[0m,  \u001b[1;36m1.8086e-01\u001b[0m,\n",
       "          \u001b[1;36m1.3459e+00\u001b[0m, \u001b[1;36m-2.2058e-01\u001b[0m, \u001b[1;36m-8.4645e-01\u001b[0m,  \u001b[1;36m1.3589e+00\u001b[0m, \u001b[1;36m-1.1583e+00\u001b[0m,\n",
       "          \u001b[1;36m8.9867e-02\u001b[0m,  \u001b[1;36m9.2684e-02\u001b[0m,  \u001b[1;36m1.6044e-03\u001b[0m, \u001b[1;36m-1.4605e+00\u001b[0m, \u001b[1;36m-1.1566e+00\u001b[0m,\n",
       "          \u001b[1;36m3.0560e-02\u001b[0m, \u001b[1;36m-1.1719e+00\u001b[0m, \u001b[1;36m-5.8213e-01\u001b[0m,  \u001b[1;36m6.6708e-01\u001b[0m,  \u001b[1;36m7.3554e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-1.0297e+00\u001b[0m,  \u001b[1;36m1.7669e+00\u001b[0m,  \u001b[1;36m1.5980e+00\u001b[0m,  \u001b[1;36m9.3797e-01\u001b[0m, \u001b[1;36m-3.8983e-01\u001b[0m,\n",
       "          \u001b[1;36m1.3899e+00\u001b[0m, \u001b[1;36m-5.9165e-01\u001b[0m, \u001b[1;36m-8.1419e-01\u001b[0m,  \u001b[1;36m1.7077e+00\u001b[0m, \u001b[1;36m-8.4548e-01\u001b[0m,\n",
       "         \u001b[1;36m-1.1995e+00\u001b[0m, \u001b[1;36m-2.9596e-01\u001b[0m, \u001b[1;36m-1.9558e-01\u001b[0m, \u001b[1;36m-6.7242e-01\u001b[0m, \u001b[1;36m-1.2226e+00\u001b[0m,\n",
       "         \u001b[1;36m-2.4787e-01\u001b[0m, \u001b[1;36m-1.5319e+00\u001b[0m, \u001b[1;36m-4.7978e-01\u001b[0m,  \u001b[1;36m4.4787e-01\u001b[0m,  \u001b[1;36m3.8553e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-7.0292e-01\u001b[0m,  \u001b[1;36m1.0196e+00\u001b[0m,  \u001b[1;36m1.2107e+00\u001b[0m,  \u001b[1;36m1.5268e+00\u001b[0m,  \u001b[1;36m1.6674e-01\u001b[0m,\n",
       "          \u001b[1;36m1.3157e+00\u001b[0m, \u001b[1;36m-2.7521e-01\u001b[0m, \u001b[1;36m-9.9856e-01\u001b[0m,  \u001b[1;36m1.1594e+00\u001b[0m, \u001b[1;36m-1.6918e+00\u001b[0m,\n",
       "          \u001b[1;36m6.9956e-02\u001b[0m, \u001b[1;36m-6.3467e-01\u001b[0m,  \u001b[1;36m6.5729e-02\u001b[0m, \u001b[1;36m-8.0846e-01\u001b[0m, \u001b[1;36m-6.7309e-01\u001b[0m,\n",
       "         \u001b[1;36m-1.4969e-01\u001b[0m, \u001b[1;36m-1.2445e+00\u001b[0m, \u001b[1;36m-1.0889e+00\u001b[0m, \u001b[1;36m-1.9688e-01\u001b[0m,  \u001b[1;36m1.4595e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-1.5217e+00\u001b[0m,  \u001b[1;36m1.5467e+00\u001b[0m,  \u001b[1;36m1.3574e+00\u001b[0m,  \u001b[1;36m4.4237e-01\u001b[0m,  \u001b[1;36m1.1169e-01\u001b[0m,\n",
       "          \u001b[1;36m1.2229e+00\u001b[0m, \u001b[1;36m-1.9101e-01\u001b[0m, \u001b[1;36m-8.9375e-01\u001b[0m,  \u001b[1;36m1.6586e+00\u001b[0m, \u001b[1;36m-6.4640e-01\u001b[0m,\n",
       "         \u001b[1;36m-9.5844e-01\u001b[0m, \u001b[1;36m-1.4530e-01\u001b[0m, \u001b[1;36m-1.3961e-01\u001b[0m, \u001b[1;36m-7.4016e-01\u001b[0m, \u001b[1;36m-1.1762e+00\u001b[0m,\n",
       "          \u001b[1;36m4.5879e-01\u001b[0m, \u001b[1;36m-1.1675e+00\u001b[0m, \u001b[1;36m-3.4486e-01\u001b[0m,  \u001b[1;36m8.3078e-02\u001b[0m, \u001b[1;36m-3.4568e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-1.6908e+00\u001b[0m,  \u001b[1;36m1.3680e+00\u001b[0m,  \u001b[1;36m1.7046e+00\u001b[0m,  \u001b[1;36m1.0478e+00\u001b[0m, \u001b[1;36m-3.4935e-02\u001b[0m,\n",
       "          \u001b[1;36m1.3482e+00\u001b[0m, \u001b[1;36m-1.1076e+00\u001b[0m, \u001b[1;36m-7.3357e-01\u001b[0m,  \u001b[1;36m1.2071e+00\u001b[0m, \u001b[1;36m-1.8945e+00\u001b[0m,\n",
       "         \u001b[1;36m-9.5916e-01\u001b[0m, \u001b[1;36m-3.1994e-01\u001b[0m, \u001b[1;36m-3.2699e-01\u001b[0m, \u001b[1;36m-1.1062e+00\u001b[0m, \u001b[1;36m-1.4718e+00\u001b[0m,\n",
       "          \u001b[1;36m8.5076e-02\u001b[0m, \u001b[1;36m-1.2185e+00\u001b[0m, \u001b[1;36m-2.6840e-01\u001b[0m,  \u001b[1;36m1.0505e+00\u001b[0m,  \u001b[1;36m5.7551e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-1.9563e-01\u001b[0m,  \u001b[1;36m1.1419e+00\u001b[0m,  \u001b[1;36m1.4114e+00\u001b[0m,  \u001b[1;36m8.3481e-01\u001b[0m, \u001b[1;36m-1.6574e-01\u001b[0m,\n",
       "          \u001b[1;36m9.8339e-01\u001b[0m,  \u001b[1;36m3.3147e-03\u001b[0m, \u001b[1;36m-1.6742e+00\u001b[0m,  \u001b[1;36m7.7683e-01\u001b[0m, \u001b[1;36m-5.6608e-01\u001b[0m,\n",
       "          \u001b[1;36m1.0877e+00\u001b[0m,  \u001b[1;36m5.8896e-02\u001b[0m, \u001b[1;36m-6.5685e-01\u001b[0m, \u001b[1;36m-3.6372e-01\u001b[0m, \u001b[1;36m-4.7666e-01\u001b[0m,\n",
       "          \u001b[1;36m5.3512e-01\u001b[0m, \u001b[1;36m-1.0576e+00\u001b[0m, \u001b[1;36m-8.8856e-01\u001b[0m,  \u001b[1;36m1.0937e-03\u001b[0m,  \u001b[1;36m3.3148e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-4.8477e-01\u001b[0m, \u001b[1;36m-1.4960e-01\u001b[0m,  \u001b[1;36m1.8474e+00\u001b[0m,  \u001b[1;36m1.2407e+00\u001b[0m, \u001b[1;36m-9.6496e-02\u001b[0m,\n",
       "          \u001b[1;36m1.0304e+00\u001b[0m, \u001b[1;36m-1.1964e-01\u001b[0m, \u001b[1;36m-1.3966e+00\u001b[0m,  \u001b[1;36m6.5855e-01\u001b[0m, \u001b[1;36m-1.3063e+00\u001b[0m,\n",
       "          \u001b[1;36m2.1486e-02\u001b[0m, \u001b[1;36m-1.6931e-01\u001b[0m, \u001b[1;36m-3.9644e-01\u001b[0m, \u001b[1;36m-6.6077e-01\u001b[0m, \u001b[1;36m-6.6220e-01\u001b[0m,\n",
       "          \u001b[1;36m1.5735e-01\u001b[0m, \u001b[1;36m-1.4622e+00\u001b[0m, \u001b[1;36m-1.0767e+00\u001b[0m, \u001b[1;36m-2.3170e-01\u001b[0m, \u001b[1;36m-5.0540e-01\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMmBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Classifier_v2(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, n_layers=12, n_head=12, n_ctx=1024, d_model=768, n_labels=20, vcb_sz=50257\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = GPT2(n_layers, n_head, n_ctx, d_model, n_labels, vcb_sz)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, src, labels=None, mask=None, pos_ids=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return self.model(src, labels, mask, pos_ids)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"labels\"],\n",
    "        )\n",
    "\n",
    "        logits = self(input_ids, labels=labels, mask=attention_mask)\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"labels\"],\n",
    "        )\n",
    "\n",
    "        logits = self(input_ids, labels=labels, mask=attention_mask)\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "            print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "            print(f\"True: {labels[0]}\")\n",
    "\n",
    "            print(f\"Pred: {preds[0]}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=2e-5)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "classifier_v2 = GPT2Classifier_v2(\n",
    "    n_layers=3, n_head=12, n_ctx=1024, d_model=768, n_labels=20, vcb_sz=50257\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    logger=False,\n",
    "    max_epochs=2,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"val_loss\"),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | model   | GPT2             | 60.7 M\n",
      "1 | loss_fn | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "60.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.7 M    Total params\n",
      "242.657   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5700f6f8fe84da2b9522d8842faa6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">From: v064mb9k@ubvmsd.cc.buffalo.edu <span style=\"font-weight: bold\">(</span>NEIL B. GANDLER<span style=\"font-weight: bold\">)</span>\n",
       "Subject: Need info on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> Bonneville\n",
       "Organization: University at Buffalo\n",
       "Lines: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "News-Software: VAX/VMS VNEWS <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.41</span>\n",
       "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
       "\n",
       "\n",
       " I am a little confused on all of the models of the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> bonnevilles.\n",
       "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
       "differences are far as features or performance. I am also curious to\n",
       "know what the book value is for prefereably the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> model. And how much\n",
       "less than book value can you usually get them for. In other words how\n",
       "much are they in demand this time of year. I have heard that the mid-spring\n",
       "early summer is the best time to buy.\n",
       "\n",
       "                        Neil Gandler\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "From: v064mb9k@ubvmsd.cc.buffalo.edu \u001b[1m(\u001b[0mNEIL B. GANDLER\u001b[1m)\u001b[0m\n",
       "Subject: Need info on \u001b[1;36m88\u001b[0m-\u001b[1;36m89\u001b[0m Bonneville\n",
       "Organization: University at Buffalo\n",
       "Lines: \u001b[1;36m10\u001b[0m\n",
       "News-Software: VAX/VMS VNEWS \u001b[1;36m1.41\u001b[0m\n",
       "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
       "\n",
       "\n",
       " I am a little confused on all of the models of the \u001b[1;36m88\u001b[0m-\u001b[1;36m89\u001b[0m bonnevilles.\n",
       "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
       "differences are far as features or performance. I am also curious to\n",
       "know what the book value is for prefereably the \u001b[1;36m89\u001b[0m model. And how much\n",
       "less than book value can you usually get them for. In other words how\n",
       "much are they in demand this time of year. I have heard that the mid-spring\n",
       "early summer is the best time to buy.\n",
       "\n",
       "                        Neil Gandler\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m: \u001b[1;36m7\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6798748bce547008c762079142a40da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faff90bf29a64f9da62063261925b592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">From: v064mb9k@ubvmsd.cc.buffalo.edu <span style=\"font-weight: bold\">(</span>NEIL B. GANDLER<span style=\"font-weight: bold\">)</span>\n",
       "Subject: Need info on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> Bonneville\n",
       "Organization: University at Buffalo\n",
       "Lines: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "News-Software: VAX/VMS VNEWS <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.41</span>\n",
       "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
       "\n",
       "\n",
       " I am a little confused on all of the models of the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> bonnevilles.\n",
       "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
       "differences are far as features or performance. I am also curious to\n",
       "know what the book value is for prefereably the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> model. And how much\n",
       "less than book value can you usually get them for. In other words how\n",
       "much are they in demand this time of year. I have heard that the mid-spring\n",
       "early summer is the best time to buy.\n",
       "\n",
       "                        Neil Gandler\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "From: v064mb9k@ubvmsd.cc.buffalo.edu \u001b[1m(\u001b[0mNEIL B. GANDLER\u001b[1m)\u001b[0m\n",
       "Subject: Need info on \u001b[1;36m88\u001b[0m-\u001b[1;36m89\u001b[0m Bonneville\n",
       "Organization: University at Buffalo\n",
       "Lines: \u001b[1;36m10\u001b[0m\n",
       "News-Software: VAX/VMS VNEWS \u001b[1;36m1.41\u001b[0m\n",
       "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
       "\n",
       "\n",
       " I am a little confused on all of the models of the \u001b[1;36m88\u001b[0m-\u001b[1;36m89\u001b[0m bonnevilles.\n",
       "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
       "differences are far as features or performance. I am also curious to\n",
       "know what the book value is for prefereably the \u001b[1;36m89\u001b[0m model. And how much\n",
       "less than book value can you usually get them for. In other words how\n",
       "much are they in demand this time of year. I have heard that the mid-spring\n",
       "early summer is the best time to buy.\n",
       "\n",
       "                        Neil Gandler\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m: \u001b[1;36m7\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: \u001b[1;36m7\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9b2d5f700c4cf09fbd96d790d2f478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">From: v064mb9k@ubvmsd.cc.buffalo.edu <span style=\"font-weight: bold\">(</span>NEIL B. GANDLER<span style=\"font-weight: bold\">)</span>\n",
       "Subject: Need info on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> Bonneville\n",
       "Organization: University at Buffalo\n",
       "Lines: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "News-Software: VAX/VMS VNEWS <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.41</span>\n",
       "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
       "\n",
       "\n",
       " I am a little confused on all of the models of the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> bonnevilles.\n",
       "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
       "differences are far as features or performance. I am also curious to\n",
       "know what the book value is for prefereably the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> model. And how much\n",
       "less than book value can you usually get them for. In other words how\n",
       "much are they in demand this time of year. I have heard that the mid-spring\n",
       "early summer is the best time to buy.\n",
       "\n",
       "                        Neil Gandler\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "From: v064mb9k@ubvmsd.cc.buffalo.edu \u001b[1m(\u001b[0mNEIL B. GANDLER\u001b[1m)\u001b[0m\n",
       "Subject: Need info on \u001b[1;36m88\u001b[0m-\u001b[1;36m89\u001b[0m Bonneville\n",
       "Organization: University at Buffalo\n",
       "Lines: \u001b[1;36m10\u001b[0m\n",
       "News-Software: VAX/VMS VNEWS \u001b[1;36m1.41\u001b[0m\n",
       "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
       "\n",
       "\n",
       " I am a little confused on all of the models of the \u001b[1;36m88\u001b[0m-\u001b[1;36m89\u001b[0m bonnevilles.\n",
       "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
       "differences are far as features or performance. I am also curious to\n",
       "know what the book value is for prefereably the \u001b[1;36m89\u001b[0m model. And how much\n",
       "less than book value can you usually get them for. In other words how\n",
       "much are they in demand this time of year. I have heard that the mid-spring\n",
       "early summer is the best time to buy.\n",
       "\n",
       "                        Neil Gandler\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m: \u001b[1;36m7\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: \u001b[1;36m13\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(classifier_v2, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Lakshya Agarwal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e102b65f27e848caa5f6a385578502ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">From: v064mb9k@ubvmsd.cc.buffalo.edu <span style=\"font-weight: bold\">(</span>NEIL B. GANDLER<span style=\"font-weight: bold\">)</span>\n",
       "Subject: Need info on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> Bonneville\n",
       "Organization: University at Buffalo\n",
       "Lines: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "News-Software: VAX/VMS VNEWS <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.41</span>\n",
       "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
       "\n",
       "\n",
       " I am a little confused on all of the models of the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> bonnevilles.\n",
       "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
       "differences are far as features or performance. I am also curious to\n",
       "know what the book value is for prefereably the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span> model. And how much\n",
       "less than book value can you usually get them for. In other words how\n",
       "much are they in demand this time of year. I have heard that the mid-spring\n",
       "early summer is the best time to buy.\n",
       "\n",
       "                        Neil Gandler\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "From: v064mb9k@ubvmsd.cc.buffalo.edu \u001b[1m(\u001b[0mNEIL B. GANDLER\u001b[1m)\u001b[0m\n",
       "Subject: Need info on \u001b[1;36m88\u001b[0m-\u001b[1;36m89\u001b[0m Bonneville\n",
       "Organization: University at Buffalo\n",
       "Lines: \u001b[1;36m10\u001b[0m\n",
       "News-Software: VAX/VMS VNEWS \u001b[1;36m1.41\u001b[0m\n",
       "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
       "\n",
       "\n",
       " I am a little confused on all of the models of the \u001b[1;36m88\u001b[0m-\u001b[1;36m89\u001b[0m bonnevilles.\n",
       "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
       "differences are far as features or performance. I am also curious to\n",
       "know what the book value is for prefereably the \u001b[1;36m89\u001b[0m model. And how much\n",
       "less than book value can you usually get them for. In other words how\n",
       "much are they in demand this time of year. I have heard that the mid-spring\n",
       "early summer is the best time to buy.\n",
       "\n",
       "                        Neil Gandler\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m: \u001b[1;36m7\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: \u001b[1;36m13\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">      Validate metric      </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span><span style=\"color: #800080; text-decoration-color: #800080\">     2.979785442352295     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    2.979785442352295    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'val_loss_epoch'\u001b[0m: \u001b[1;36m2.979785442352295\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(classifier_v2, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
